{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wisdom-24/Business-intelligence/blob/main/week6_%E5%88%86%E6%9E%90%E9%87%91%E8%9E%8D%E6%83%85%E7%B7%92%EF%BC%BF%E5%88%A9%E7%94%A8bert%E6%A8%A1%E5%9E%8B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY_4q-X807M6"
      },
      "source": [
        "Hugging Face\n",
        "Hugging Face 網址：https://huggingface.co/\n",
        "\n",
        "Hugging Face 是 AI 領域的開放平台，像 GitHub 一樣託管並分享 AI 模型與資料集。也收錄了很多來自頂尖研究的模型，涵蓋 NLP、計算機視覺、語音處理等領域。使用者可以在 Hub 上存取、分享、微調模型，加速 AI 的合作與創新。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0VCHhjU8V2o",
        "outputId": "9c31b2cf-7210-42e2-af5e-c8c783964c27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dnMJlQb8V2p",
        "outputId": "1aa8816f-df1b-4184-d7e1-61c71d7cf478"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/中興資管所/商業智慧與營運管理/week6 分析金融情緒＿利用bert模型.ipynb\n"
          ]
        }
      ],
      "source": [
        "!find /content -name \"week6 分析金融情緒＿利用bert模型.ipynb\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yi60gMUM8V2p",
        "outputId": "d36f1491-16e9-4157-df21-d26ca88c1213"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook /content/drive/MyDrive/中興資管所/商業智慧與營運管理/week6 分析金融情緒＿利用bert模型.ipynb to notebook\n",
            "[NbConvertApp] Writing 82129 bytes to /content/drive/MyDrive/中興資管所/商業智慧與營運管理/week6 分析金融情緒＿利用bert模型.ipynb\n"
          ]
        }
      ],
      "source": [
        "!jupyter nbconvert --ClearMetadataPreprocessor.enabled=True --inplace --to notebook '/content/drive/MyDrive/中興資管所/商業智慧與營運管理/week6 分析金融情緒＿利用bert模型.ipynb'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1UPxqHk8V2p",
        "outputId": "9280b234-b86e-4397-8169-4892c7ff9c55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.35.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "- datasets 是 Hugging Face 提供的資料集載入工具\n",
        "- 它整合了數千個公開資料集，包含 NLP、CV、Audio 等領域\n",
        "- 提供統一的介面來下載、處理和使用資料集\n",
        "- 支援快取機制，避免重複下載\n",
        "\"\"\"\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYt8XvAt1Jt0"
      },
      "source": [
        "資料前處理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "3e954fe25e7844a5b3a1117bd73cde8f",
            "8735e028ef2c4ef9bddaef2ddba3a97b",
            "035bfe118ad349d58fd0704bcb6a7963",
            "c9c4d7cdcf224116a0da6d9d7b4c576e",
            "88bc61bfb50f4a1cac3177cbc5dfa255",
            "3d4274dff6044f3ea60116e342eb4627"
          ]
        },
        "id": "1XxDo-_W8V2p",
        "outputId": "ee2d6b44-fd55-444a-8c9c-d6a571ca440c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e954fe25e7844a5b3a1117bd73cde8f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8735e028ef2c4ef9bddaef2ddba3a97b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "dataset_infos.json:   0%|          | 0.00/800 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "035bfe118ad349d58fd0704bcb6a7963",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00000-of-00001.parquet:   0%|          | 0.00/327k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9c4d7cdcf224116a0da6d9d7b4c576e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/test-00000-of-00001.parquet:   0%|          | 0.00/80.9k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88bc61bfb50f4a1cac3177cbc5dfa255",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/3877 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d4274dff6044f3ea60116e342eb4627",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/969 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['sentence', 'label'],\n",
            "        num_rows: 3877\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['sentence', 'label'],\n",
            "        num_rows: 969\n",
            "    })\n",
            "})\n",
            "訓練集： Dataset({\n",
            "    features: ['sentence', 'label'],\n",
            "    num_rows: 3877\n",
            "})\n",
            "測試集： Dataset({\n",
            "    features: ['sentence', 'label'],\n",
            "    num_rows: 969\n",
            "})\n",
            "訓練集欄位： ['sentence', 'label']\n",
            "測試集欄位： ['sentence', 'label']\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "\"\"\"\n",
        "為什麼選擇 auditor_sentiment 資料集？\n",
        "1. 金融領域專業性：專門針對財務報告和審計意見設計\n",
        "2. 標籤品質：由金融專家標註，確保情緒分類的準確性\n",
        "3. 真實場景：模擬實際金融分析師的工作情境\n",
        "4. 三分類問題：Negative(負面)、Neutral(中性)、Positive(正面)\n",
        "\n",
        "這個資料集適合用來訓練金融新聞情緒分析模型，\n",
        "\"\"\"\n",
        "\n",
        "# 使用已轉換好的版本\n",
        "dataset = load_dataset(\n",
        "    \"FinanceInc/auditor_sentiment\",  # 這是轉換後的版本\n",
        "    #split=\"train\"\n",
        ")\n",
        "\n",
        "print(dataset)\n",
        "\n",
        "print(\"訓練集：\", dataset['train'])\n",
        "print(\"測試集：\", dataset['test'])\n",
        "\n",
        "# 查看資料集的欄位名稱與類型\n",
        "print(\"訓練集欄位：\", dataset[\"train\"].column_names)\n",
        "print(\"測試集欄位：\", dataset[\"test\"].column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3YmIR118V2q",
        "outputId": "09198c58-0a4d-4c46-ad8f-ee8a273aac6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "訓練集前5筆：\n",
            "{'sentence': [\"Altia 's operating profit jumped to EUR 47 million from EUR 6.6 million .\", 'The agreement was signed with Biohit Healthcare Ltd , the UK-based subsidiary of Biohit Oyj , a Finnish public company which develops , manufactures and markets liquid handling products and diagnostic test systems .', 'Kesko pursues a strategy of healthy , focused growth concentrating on sales and services to consumer-customers .', 'Vaisala , headquartered in Helsinki in Finland , develops and manufactures electronic measurement systems for meteorology , environmental sciences , traffic and industry .', 'Also , a six-year historic analysis is provided for these markets .'], 'label': [2, 2, 2, 1, 1]}\n",
            "\n",
            "測試集前5筆：\n",
            "{'sentence': [\"TeliaSonera TLSN said the offer is in line with its strategy to increase its ownership in core business holdings and would strengthen Eesti Telekom 's offering to its customers .\", 'STORA ENSO , NORSKE SKOG , M-REAL , UPM-KYMMENE Credit Suisse First Boston ( CFSB ) raised the fair value for shares in four of the largest Nordic forestry groups .', \"Clothing retail chain Sepp+ñl+ñ 's sales increased by 8 % to EUR 155.2 mn , and operating profit rose to EUR 31.1 mn from EUR 17.1 mn in 2004 .\", 'Lifetree was founded in 2000 , and its revenues have risen on an average by 40 % with margins in late 30s .', \"Nordea Group 's operating profit increased in 2010 by 18 percent year-on-year to 3.64 billion euros and total revenue by 3 percent to 9.33 billion euros .\"], 'label': [2, 2, 2, 2, 2]}\n",
            "\n",
            "資料集的 keys： dict_keys(['train', 'test'])\n"
          ]
        }
      ],
      "source": [
        "# 查看訓練集中前幾筆數據\n",
        "print(\"\\n訓練集前5筆：\")\n",
        "print(dataset[\"train\"][:5])\n",
        "\n",
        "# 查看測試集中前幾筆數據\n",
        "print(\"\\n測試集前5筆：\")\n",
        "print(dataset[\"test\"][:5])\n",
        "\n",
        "# 查看資料集中有哪些 Keys\n",
        "print(\"\\n資料集的 keys：\", dataset.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoEzHi1X8V2r",
        "outputId": "abad102b-780f-41bd-fb73-fe37aecb2adb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "訓練集標籤分佈： Counter({1: 2320, 2: 1077, 0: 480})\n",
            "測試集標籤分佈： Counter({1: 559, 2: 286, 0: 124})\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter # 計算列表中每個元素的出現次數\n",
        "\n",
        "\"\"\"\n",
        "為什麼要分析標籤分佈？\n",
        "1. 檢查類別不平衡(Class Imbalance)問題\n",
        "   - 如果某類別樣本過少，模型可能學不好該類別\n",
        "   - 可能需要使用過採樣(oversampling)或欠採樣(undersampling)\n",
        "\n",
        "2. 了解資料特性\n",
        "   - 金融文本中，中性(Neutral)通常佔多數\n",
        "   - 正面和負面情緒相對較少\n",
        "\n",
        "3. 決定評估指標\n",
        "   - 如果類別不平衡嚴重，準確率(Accuracy)可能不適合\n",
        "   - 應該使用 F1-score、Precision、Recall 等指標\n",
        "\"\"\"\n",
        "# label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
        "# 計算訓練集標籤出現次數\n",
        "label_counts_train = Counter(dataset[\"train\"][\"label\"])\n",
        "print(\"\\n訓練集標籤分佈：\", label_counts_train)\n",
        "\n",
        "# 計算測試集標籤出現次數\n",
        "label_counts_test = Counter(dataset[\"test\"][\"label\"])\n",
        "print(\"測試集標籤分佈：\", label_counts_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Jea40U78V2s",
        "outputId": "95d6cc08-09d4-4592-fdfe-af2e0e02cdae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "訓練集前5筆：\n",
            "                                            sentence  label\n",
            "0  Altia 's operating profit jumped to EUR 47 mil...      2\n",
            "1  The agreement was signed with Biohit Healthcar...      2\n",
            "2  Kesko pursues a strategy of healthy , focused ...      2\n",
            "3  Vaisala , headquartered in Helsinki in Finland...      1\n",
            "4  Also , a six-year historic analysis is provide...      1\n",
            "\n",
            "測試集前5筆：\n",
            "                                            sentence  label\n",
            "0  TeliaSonera TLSN said the offer is in line wit...      2\n",
            "1  STORA ENSO , NORSKE SKOG , M-REAL , UPM-KYMMEN...      2\n",
            "2  Clothing retail chain Sepp+ñl+ñ 's sales incre...      2\n",
            "3  Lifetree was founded in 2000 , and its revenue...      2\n",
            "4  Nordea Group 's operating profit increased in ...      2\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd  # 結構化資料處理套件\n",
        "\n",
        "\"\"\"\n",
        "為什麼要轉換成 DataFrame？\n",
        "1. 更直觀的資料檢視：表格形式更容易閱讀和理解\n",
        "2. 豐富的資料處理功能：pandas 提供大量資料清洗工具\n",
        "3. 方便進行統計分析：可以快速計算統計量\n",
        "4. 易於視覺化：可以搭配 matplotlib、seaborn 繪圖\n",
        "\"\"\"\n",
        "\n",
        "# 轉換為 DataFrame\n",
        "df_train = pd.DataFrame(dataset[\"train\"])\n",
        "df_test = pd.DataFrame(dataset[\"test\"])\n",
        "\n",
        "print(\"\\n訓練集前5筆：\")\n",
        "print(df_train.head())\n",
        "\n",
        "print(\"\\n測試集前5筆：\")\n",
        "print(df_test.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VS7SHBrf8V2s",
        "outputId": "67f0dd5c-f024-4aed-f6b4-5c15d760aea3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "訓練集空值： sentence    0\n",
            "label       0\n",
            "dtype: int64\n",
            "測試集空值： sentence    0\n",
            "label       0\n",
            "dtype: int64\n",
            "\n",
            "訓練集大小： (3872, 2)\n",
            "測試集大小： (969, 2)\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "為什麼要檢查空值和重複值？\n",
        "1. 空值(Null values)問題\n",
        "   - 空值會導致模型訓練失敗或產生錯誤\n",
        "   - 需要決定是刪除還是填補(imputation)\n",
        "\n",
        "2. 重複值(Duplicates)問題\n",
        "   - 重複的句子會讓模型過度擬合該樣本\n",
        "   - 影響模型的泛化能力\n",
        "   - 可能導致評估指標不準確\n",
        "\"\"\"\n",
        "# 檢查空值\n",
        "print(\"\\n訓練集空值：\", df_train.isnull().sum())\n",
        "print(\"測試集空值：\", df_test.isnull().sum())\n",
        "\n",
        "# 刪除重複值（如果有的話）\n",
        "df_train.drop_duplicates(subset=[\"sentence\"], inplace=True)\n",
        "df_test.drop_duplicates(subset=[\"sentence\"], inplace=True)\n",
        "\n",
        "print(\"\\n訓練集大小：\", df_train.shape)\n",
        "print(\"測試集大小：\", df_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0t63vEmX8V2s",
        "outputId": "f2bae314-0e1e-48f4-b4ed-7d944d5cbdec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "資料集劃分：\n",
            "訓練集大小：3101\n",
            "驗證集大小：776 （從訓練集切出，用於訓練過程中的評估）\n",
            "測試集大小：969 （保留到最後才使用，避免資訊洩漏）\n"
          ]
        }
      ],
      "source": [
        "train_val_split = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
        "train_dataset = train_val_split[\"train\"]\n",
        "val_dataset = train_val_split[\"test\"]\n",
        "test_dataset = dataset[\"test\"]\n",
        "print(\"\\n資料集劃分：\")\n",
        "print(f\"訓練集大小：{len(train_dataset)}\")\n",
        "print(f\"驗證集大小：{len(val_dataset)} （從訓練集切出，用於訓練過程中的評估）\")\n",
        "print(f\"測試集大小：{len(test_dataset)} （保留到最後才使用，避免資訊洩漏）\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "9a46768a02d64106ab137a90b22f2da8",
            "419762f915f848f9971013a8b6e9f9cf",
            "cac3ab56f0744ceab300975ec115303d",
            "aa756c34272046c5a34f16a87a7d00c7",
            "83b398e96b034bdea0e7e811815911bb",
            "b8003fc0ff6d447b9c9df707021519b5",
            "03b1d8ce0c1b4f249587406f49ab29ad",
            "77663fa592a84105b45fe65dd6512278"
          ]
        },
        "id": "X_f9JIQk8V2t",
        "outputId": "7684319f-4408-4bb5-faeb-e3f2ada09911"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "使用的模型：roberta-base\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a46768a02d64106ab137a90b22f2da8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "419762f915f848f9971013a8b6e9f9cf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cac3ab56f0744ceab300975ec115303d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa756c34272046c5a34f16a87a7d00c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83b398e96b034bdea0e7e811815911bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8003fc0ff6d447b9c9df707021519b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/3101 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "03b1d8ce0c1b4f249587406f49ab29ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/776 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77663fa592a84105b45fe65dd6512278",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/969 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer  # AutoTokenizer 可以根據指定的模型名稱，自動選擇和載入與該模型相匹配的分詞器\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "=============================================================================\n",
        "【作業 1：模型選擇】\n",
        "=============================================================================\n",
        "\n",
        "什麼是 Tokenizer？\n",
        "- 將文本轉換成模型可以理解的數字序列\n",
        "- 不同模型有不同的 tokenization 策略\n",
        "- BERT 使用 WordPiece，將詞切成子詞單元\n",
        "\n",
        "可選模型比較：\n",
        "\n",
        "1. bert-base-uncased (預設)\n",
        "   優點：經典模型，效果穩定，社群資源豐富\n",
        "   缺點：模型較大(110M參數)，訓練速度較慢\n",
        "   適用：標準 NLP 任務，對效果要求高\n",
        "\n",
        "2. distilbert-base-uncased\n",
        "   優點：BERT 的蒸餾版本，速度快 60%，模型小 40%\n",
        "   缺點：準確率略低於 BERT (約 2-3%)\n",
        "   適用：計算資源有限，需要快速迭代\n",
        "\n",
        "3. roberta-base\n",
        "   優點：改進的 BERT，通常效果更好\n",
        "   缺點：訓練時間稍長\n",
        "   適用：追求更高準確率\n",
        "\n",
        "4. albert-base-v2\n",
        "   優點：參數共享技術，模型很小但效果不錯\n",
        "   缺點：推理速度較慢\n",
        "   適用：記憶體受限的場景\n",
        "\n",
        "5. ProsusAI/finbert\n",
        "   優點：專門在金融文本上預訓練，理解金融術語\n",
        "   缺點：資料集較小，可能過擬合\n",
        "   適用：金融領域專業任務（最推薦！）\n",
        "\n",
        "6. bert-base-chinese\n",
        "   優點：支援中文\n",
        "   缺點：英文效果不佳\n",
        "   適用：中文金融文本分析\n",
        "\n",
        "評估面向：\n",
        "✓ 訓練時間\n",
        "✓ 驗證集 F1-score\n",
        "✓ 測試集 F1-score\n",
        "✓ 模型大小\n",
        "✓ 推理速度\n",
        "\"\"\"\n",
        "# 使用BERT Tokenizer 進行Tokenization(分詞)\n",
        "model_name = \"roberta-base\"  # 請修改這裡！\n",
        "\n",
        "print(f\"\\n使用的模型：{model_name}\")\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "為什麼要使用 AutoTokenizer？\n",
        "- 自動根據模型名稱載入對應的 tokenizer\n",
        "- 確保 tokenization 策略與模型匹配\n",
        "- 避免手動管理不同模型的 tokenizer\n",
        "\"\"\"\n",
        "from transformers import AutoTokenizer  # AutoTokenizer 可以根據指定的模型名稱，自動選擇和載入與該模型相匹配的分詞器\n",
        "\n",
        "# 使用指定的 Tokenizer 進行 Tokenization(分詞)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Tokenization 的關鍵參數：\n",
        "\n",
        "1. padding=\"max_length\"\n",
        "   - 將所有句子補齊到相同長度\n",
        "   - 原因：深度學習模型要求批次(batch)內的輸入長度一致\n",
        "   - 較短的句子會用 [PAD] token 填充\n",
        "\n",
        "2. truncation=True\n",
        "   - 截斷超過最大長度的句子\n",
        "   - BERT 的最大長度通常是 512 tokens\n",
        "   - 避免超長句子導致記憶體不足\n",
        "\n",
        "3. return_tensors=\"pt\" (後面會用到)\n",
        "   - 返回 PyTorch tensor 格式\n",
        "   - 可以直接輸入模型進行運算\n",
        "\"\"\"\n",
        "\n",
        "# 定義 Tokenize 函數，用於將句子轉換為 Token\n",
        "def tokenize_function(example):\n",
        "    # 使用 tokenizer 將每個句子進行分詞\n",
        "    # 設定 padding=\"max_length\" 確保所有句子補齊至固定長度\n",
        "    # 設定 truncation=True 以截斷超過最大長度的句子\n",
        "    return tokenizer(example[\"sentence\"], padding=\"max_length\", truncation=True)\n",
        "\"\"\"\n",
        "Tokenization 結果說明：\n",
        "\n",
        "input_ids：\n",
        "- 將文字轉換成數字 ID\n",
        "- 例如：[101, 2043, 2003, 1996, ...]\n",
        "- 101 是 [CLS] token，表示句子開始\n",
        "- 102 是 [SEP] token，表示句子結束\n",
        "\n",
        "attention_mask：\n",
        "- 標記哪些位置是真實 token (1)，哪些是 padding (0)\n",
        "- 讓模型知道應該關注哪些部分\n",
        "\n",
        "token_type_ids：\n",
        "- 區分不同句子（在句子對任務中使用）\n",
        "- 在單句分類中全部為 0\n",
        "\"\"\"\n",
        "# 對訓練集、驗證集和測試集進行 Tokenize\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_val = val_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test = test_dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYQnmmvb8V2t",
        "outputId": "6c09aaa8-bcbe-4f4f-e793-d1c1a7838901"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Tokenized 訓練集第一筆：\n",
            "{'sentence': [\"The financial impact is estimated to be some 1.5 MEUR annual improvement in the division 's result , starting from fiscal year 2007 .\"], 'label': [2], 'input_ids': [[0, 133, 613, 913, 16, 2319, 7, 28, 103, 112, 4, 245, 12341, 2492, 1013, 3855, 11, 5, 2757, 128, 29, 898, 2156, 1158, 31, 2358, 76, 3010, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n",
            "\n",
            "Tokenized 驗證集第一筆：\n",
            "{'sentence': [\"We have also cut our price projections for paper and packaging , '' an analyst with Goldman Sachs said on a note on Monday .\"], 'label': [0], 'input_ids': [[0, 170, 33, 67, 847, 84, 425, 8979, 13, 2225, 8, 9805, 2156, 12801, 41, 2066, 19, 6765, 8922, 26, 15, 10, 1591, 15, 302, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n",
            "\n",
            "Tokenized 測試集第一筆：\n",
            "{'sentence': [\"TeliaSonera TLSN said the offer is in line with its strategy to increase its ownership in core business holdings and would strengthen Eesti Telekom 's offering to its customers .\"], 'label': [2], 'input_ids': [[0, 565, 30922, 29111, 3843, 46402, 487, 26, 5, 904, 16, 11, 516, 19, 63, 1860, 7, 712, 63, 4902, 11, 2731, 265, 4582, 8, 74, 5920, 381, 990, 118, 12024, 330, 1075, 128, 29, 1839, 7, 63, 916, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n",
            "\n",
            "訓練集特徵： {'sentence': Value('string'), 'label': Value('int64'), 'input_ids': List(Value('int32')), 'attention_mask': List(Value('int8'))}\n",
            "驗證集特徵： {'sentence': Value('string'), 'label': Value('int64'), 'input_ids': List(Value('int32')), 'attention_mask': List(Value('int8'))}\n",
            "測試集特徵： {'sentence': Value('string'), 'label': Value('int64'), 'input_ids': List(Value('int32')), 'attention_mask': List(Value('int8'))}\n"
          ]
        }
      ],
      "source": [
        "# 查看 Tokenized 資料\n",
        "print(\"\\nTokenized 訓練集第一筆：\")\n",
        "print(tokenized_train[:1])\n",
        "\n",
        "print(\"\\nTokenized 驗證集第一筆：\")\n",
        "print(tokenized_val[:1])\n",
        "\n",
        "print(\"\\nTokenized 測試集第一筆：\")\n",
        "print(tokenized_test[:1])\n",
        "# 確認 tokenized_datasets 是否包含需要的欄位\n",
        "print(\"\\n訓練集特徵：\", tokenized_train.features)\n",
        "print(\"驗證集特徵：\", tokenized_val.features)\n",
        "print(\"測試集特徵：\", tokenized_test.features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdZV5JiD4f3s"
      },
      "source": [
        "建立模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYwQ95m28V2t",
        "outputId": "efd1a1a9-bfd0-4ea2-a7b0-57b9d81858ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch  # 深度學習框架\n",
        "\"\"\"\n",
        "為什麼要檢查 GPU？\n",
        "- GPU 的平行運算能力遠超 CPU（通常快 10-100 倍）\n",
        "- 深度學習涉及大量矩陣運算，適合 GPU 加速\n",
        "- 如果有 GPU 可用，應該優先使用\n",
        "\n",
        "CUDA 是什麼？\n",
        "- NVIDIA 的平行運算平台\n",
        "- PyTorch 透過 CUDA 調用 GPU\n",
        "- torch.cuda.is_available() 檢查是否有可用的 GPU\n",
        "\"\"\"\n",
        "# 如果有GPU就用GPU，沒有GPU用CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "eae513e577ec4d048a31d502de91c3c9"
          ]
        },
        "id": "GSngvcvh8V2t",
        "outputId": "8369c42a-9406-4fff-de6e-6ef099e2a490"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eae513e577ec4d048a31d502de91c3c9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification  # 序列分類模型\n",
        "\"\"\"\n",
        "什麼是遷移學習(Transfer Learning)？\n",
        "\n",
        "1. 預訓練(Pre-training)階段\n",
        "   - 在大規模文本語料上預訓練\n",
        "   - 學習通用的語言理解能力\n",
        "   - BERT 在 BookCorpus 和 Wikipedia 上訓練\n",
        "\n",
        "2. 微調(Fine-tuning)階段\n",
        "   - 在特定任務上微調模型\n",
        "   - 只需要較少的標註資料\n",
        "   - 適應特定領域（如金融文本）\n",
        "\n",
        "為什麼設定 num_labels=3？\n",
        "- 我們的任務是三分類：Negative, Neutral, Positive\n",
        "- 模型最後會輸出 3 個分數(logits)\n",
        "- 透過 softmax 轉換成機率分佈\n",
        "\n",
        "model.to(device) 的作用？\n",
        "- 將模型參數移動到 GPU 上\n",
        "- 確保模型和輸入資料在同一個裝置\n",
        "\"\"\"\n",
        "\n",
        "# 載入模型（使用作業區域 1 選擇的模型）\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
        "model.to(device)  # 將模型移到對應裝置"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80wpFSOm8V2t"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments  # 包裝訓練參數\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",          # 訓練結果的儲存位置，包括模型檔案和預測結果。\n",
        "    eval_strategy=\"epoch\",     # 設置評估策略為每個訓練輪次（epoch）結束後進行一次評估。\n",
        "    learning_rate=2e-5,              # 設置學習率，這裡使用 2e-5，通常較小的學習率更適合微調預訓練模型。\n",
        "    per_device_train_batch_size=4,   # 設定每個裝置（如每張 GPU）的訓練批次大小為 4。\n",
        "    per_device_eval_batch_size=4,    # 設定每個裝置的評估批次大小為 4。\n",
        "    num_train_epochs=3,              # 訓練輪次設定為 3，模型將完整遍歷訓練集三次。\n",
        "    weight_decay=0.01,               # 設置權重衰減（L2正則化）參數，這裡為 0.01，用於防止過擬合。\n",
        "    logging_dir=\"./logs\",            # 日誌存放目錄，用於儲存 TensorBoard 或其他日誌。\n",
        "    logging_steps=10,                # 設置每 10 個步驟記錄一次訓練信息，便於監控訓練過程。\n",
        "    report_to=\"none\",                 # 不將訓練日誌發送到任何平臺（如 TensorBoard），可以改為 \"tensorboard\" 以啟用。\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKafWcfX8V2u"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "\"\"\"\n",
        "為什麼需要這些評估指標？\n",
        "\n",
        "1. Accuracy (準確率)\n",
        "   公式：(預測正確的樣本數) / (總樣本數)\n",
        "   優點：直觀易懂\n",
        "   缺點：在類別不平衡時會誤導\n",
        "\n",
        "2. Precision (精確率)\n",
        "   公式：(真正例) / (真正例 + 假正例)\n",
        "   意義：預測為正例的樣本中，真正為正例的比例\n",
        "   應用：當誤報代價高時重視精確率\n",
        "\n",
        "3. Recall (召回率)\n",
        "   公式：(真正例) / (真正例 + 假負例)\n",
        "   意義：所有正例中，被正確預測的比例\n",
        "   應用：當漏報代價高時重視召回率\n",
        "\n",
        "4. F1-score (F1分數)\n",
        "   公式：2 * (Precision * Recall) / (Precision + Recall)\n",
        "   意義：精確率和召回率的調和平均\n",
        "   優點：綜合考慮兩者，適合類別不平衡\n",
        "\n",
        "為什麼使用 average=\"weighted\"？\n",
        "- 根據每個類別的樣本數加權平均\n",
        "- 考慮類別不平衡的影響\n",
        "- 更能反映整體表現\n",
        "\n",
        "在金融情緒分析中的重要性：\n",
        "- Precision 高：減少誤判（例如誤把中性當成正面）\n",
        "- Recall 高：不錯過重要信號（例如不漏掉負面新聞）\n",
        "- F1-score 高：整體表現好\n",
        "\"\"\"\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids  # 取得真實值的id\n",
        "    preds = np.argmax(pred.predictions, axis=1)  # 預測值找到最高機率的索引\n",
        "    accuracy = accuracy_score(labels, preds)  # 計算準確率\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\")  # 計算其他指標\n",
        "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KenvBxQ08V2u",
        "outputId": "cb610401-1c42-4712-f311-d5033f050b16"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2328' max='2328' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2328/2328 18:17, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.166400</td>\n",
              "      <td>0.603498</td>\n",
              "      <td>0.864691</td>\n",
              "      <td>0.864028</td>\n",
              "      <td>0.864691</td>\n",
              "      <td>0.862975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.503000</td>\n",
              "      <td>0.676377</td>\n",
              "      <td>0.860825</td>\n",
              "      <td>0.863412</td>\n",
              "      <td>0.860825</td>\n",
              "      <td>0.861587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.147200</td>\n",
              "      <td>0.838037</td>\n",
              "      <td>0.862113</td>\n",
              "      <td>0.865722</td>\n",
              "      <td>0.862113</td>\n",
              "      <td>0.863151</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2328, training_loss=0.4268115549076259, metrics={'train_runtime': 1099.2135, 'train_samples_per_second': 8.463, 'train_steps_per_second': 2.118, 'total_flos': 2447744125123584.0, 'train_loss': 0.4268115549076259, 'epoch': 3.0})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "\"\"\"\n",
        "Trainer 類別的作用：\n",
        "- 封裝了完整的訓練流程\n",
        "- 自動處理：\n",
        "  ✓ 梯度計算和參數更新\n",
        "  ✓ 批次資料載入\n",
        "  ✓ 學習率調整\n",
        "  ✓ 驗證集評估\n",
        "  ✓ Checkpoint 儲存\n",
        "  ✓ 日誌記錄\n",
        "\n",
        "為什麼使用高階 API？\n",
        "- 減少樣板代碼(boilerplate code)\n",
        "- 避免常見錯誤\n",
        "- 專注於模型和資料\n",
        "- 便於快速實驗\n",
        "\n",
        "訓練過程中會發生什麼？\n",
        "1. 前向傳播(Forward pass)\n",
        "   - 輸入資料經過模型，得到預測結果\n",
        "\n",
        "2. 計算損失(Loss calculation)\n",
        "   - 比較預測結果和真實標籤\n",
        "   - 使用交叉熵損失(Cross-entropy loss)\n",
        "\n",
        "3. 反向傳播(Backward pass)\n",
        "   - 計算損失對參數的梯度\n",
        "\n",
        "4. 參數更新(Parameter update)\n",
        "   - 使用梯度和學習率更新參數\n",
        "   - 優化器：AdamW (Adam with weight decay)\n",
        "\n",
        "5. 驗證評估(Validation)\n",
        "   - 每個 epoch 結束後在驗證集上評估\n",
        "   - 不更新參數，只計算指標\n",
        "\"\"\"\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,  # 使用訓練集\n",
        "    eval_dataset=tokenized_val,     # 使用驗證集（從訓練集切出來的）\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# 開始訓練\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0XR1AM0jQ-3"
      },
      "source": [
        "結果評估"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9vAfjui8V2u",
        "outputId": "9e8b0263-0b62-43f0-edc9-7d20fded496b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== 驗證集評估結果 ===\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='194' max='194' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [194/194 00:22]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.8380370736122131, 'eval_accuracy': 0.8621134020618557, 'eval_precision': 0.8657223212606738, 'eval_recall': 0.8621134020618557, 'eval_f1': 0.8631512503041503, 'eval_runtime': 22.6163, 'eval_samples_per_second': 34.312, 'eval_steps_per_second': 8.578, 'epoch': 3.0}\n"
          ]
        }
      ],
      "source": [
        "# 評估模型在驗證集的表現\n",
        "\n",
        "print(\"\\n=== 驗證集評估結果 ===\")\n",
        "val_result = trainer.evaluate(eval_dataset=tokenized_val)\n",
        "print(val_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hLxNsv08V2u",
        "outputId": "3bda8b08-92df-4cd3-949b-7a1e724d9258"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== 測試集評估結果（最終評估）===\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='437' max='194' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [194/194 01:10]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.653393566608429, 'eval_accuracy': 0.8771929824561403, 'eval_precision': 0.8788744532224037, 'eval_recall': 0.8771929824561403, 'eval_f1': 0.877577533192376, 'eval_runtime': 27.6209, 'eval_samples_per_second': 35.082, 'eval_steps_per_second': 8.798, 'epoch': 3.0}\n"
          ]
        }
      ],
      "source": [
        "# 評估模型在測試集的表現（最終評估）\n",
        "print(\"\\n=== 測試集評估結果（最終評估）===\")\n",
        "test_result = trainer.evaluate(eval_dataset=tokenized_test)\n",
        "print(test_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1l434sHkScB"
      },
      "source": [
        "使用測試集進行預測展示"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejEMGLBN8V2v",
        "outputId": "b4be4ea8-1f5e-4e1e-d053-0c511a13ae0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== 測試集句子展示 ===\n",
            "以下是測試集中的前 50 個句子：\n",
            "\n",
            "[0] TeliaSonera TLSN said the offer is in line with its strategy to increase its ownership in core business holdings and would strengthen Eesti Telekom 's offering to its customers .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[1] STORA ENSO , NORSKE SKOG , M-REAL , UPM-KYMMENE Credit Suisse First Boston ( CFSB ) raised the fair value for shares in four of the largest Nordic forestry groups .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[2] Clothing retail chain Sepp+ñl+ñ 's sales increased by 8 % to EUR 155.2 mn , and operating profit rose to EUR 31.1 mn from EUR 17.1 mn in 2004 .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[3] Lifetree was founded in 2000 , and its revenues have risen on an average by 40 % with margins in late 30s .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[4] Nordea Group 's operating profit increased in 2010 by 18 percent year-on-year to 3.64 billion euros and total revenue by 3 percent to 9.33 billion euros .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[5] Operating profit for the nine-month period increased from EUR3 .1 m and net sales increased from EUR61 .5 m , as compared to the corresponding period in 2007 .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[6] The Lithuanian beer market made up 14.41 million liters in January , a rise of 0.8 percent from the year-earlier figure , the Lithuanian Brewers ' Association reporting citing the results from its members .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[7] In January-September 2010 , Fiskars ' net profit went up by 14 % year-on-year to EUR 65.4 million and net sales to EUR 525.3 million from EUR 487.7 million .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[8] Net income from life insurance rose to EUR 16.5 mn from EUR 14.0 mn , and net income from non-life insurance to EUR 22.6 mn from EUR 15.2 mn in 2009 .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[9] Nyrstar has also agreed to supply to Talvivaara up to 150,000 tonnes of sulphuric acid per annum for use in Talvivaara 's leaching process during the period of supply of the zinc in concentrate .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[10] The agreement strengthens our long-term partnership with Nokia Siemens Networks .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[11] KESKO CORPORATION STOCK EXCHANGE RELEASE 28.02.2008 AT 09.30 1 ( 1 ) Kesko Corporation and Aspo plc today signed an agreement by which Aspo acquires the share capital of Kauko-Telko Ltd , a subsidiary wholly owned by Kesko .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[12] The OMX Helsinki 25 index was up 0.92 pct at 2,518.67 and the Helsinki CAP portfolio index was 0.91 pct higher at 4,711.19 .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[13] Tiimari operates 194 stores in six countries -- including its core Finnish market -- and generated a turnover of 76.5 mln eur in 2005 .\n",
            "    真實標籤：Neutral\n",
            "\n",
            "[14] Under this agreement Biohit becomes a focus supplier of pipettors and disposable pipettor tips to VWR customers throughout Europe .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[15] Adjusted for changes in the Group structure , the Division 's net sales increased by 1.7 % .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[16] Finnish Aktia Group 's operating profit rose to EUR 17.5 mn in the first quarter of 2010 from EUR 8.2 mn in the first quarter of 2009 .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[17] Finnish high technology provider Vaahto Group reports net sales of EUR 41.8 mn in the accounting period September 2007 - February 2008 , an increase of 11.2 % from a year earlier .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[18] Biohit already services many current Genesis customers and the customer base is expected to expand as a result of this agreement .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[19] Circulation revenue has increased by 5 % in Finland and 4 % in Sweden in 2008 .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[20] In 2009 , Fiskars ' cash flow from operating activities amounted to EUR121m , up from EUR97m in the previous year .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[21] Operating profit margin increased from 11.2 % to 11.7 % .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[22] The circuit 's overall production rate on a weekly basis is now in excess of an average of 40,000 tonnes per day , with volumes in excess of 50,000 tonnes per day being reached on individual days .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[23] The last quarter was the best quarter of 2009 in net sales , and the operating margin rose to 12.2 % .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[24] The transaction doubles Tecnomens workforse , and adds a fourth to their net sales .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[25] `` The priority for 2009 was to strengthen the company 's balance sheet and increase cash flow , '' CEO Hannu Krook said .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[26] A structures BIM ( building information modeling ) software from Tekla , a model-based software provider , has been adopted for the construction management and delivery of a university project in Abu Dhabi .\n",
            "    真實標籤：Neutral\n",
            "\n",
            "[27] According to Finnish Metso Minerals , the value of the company 's orders has gone up to EUR 1.9 bn in 12 months .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[28] Finnish forest machinery manufacturer Ponsse 's net sales grew to EUR 51.3 mn in the first quarter of 2010 from EUR 37.5 mn in the corresponding period in 2009 .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[29] Finnish KCI Konecranes has raised its net sales growth estimate for 2006 from over 25 % to over 35 % .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[30] Finnish pharmaceuticals company Orion 's net sales rose to EUR 190mn in the first quarter of 2009 from EUR 180mn in the first quarter of 2008 .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[31] Again , the most significant sales increase of 18.6 % was in Russia .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[32] Also Lemmink+ñinen 's profit for accounting period went up to EUR 3.1 mn from EUR -24.5 mn a year ago .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[33] Besides we have increased the share of meat in various sausages and are offering a number of new tastes in the grill products and shish kebabs segment , '' Paavel said .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[34] Both operating profit and net sales for the three-month period increased , respectively from EUR16 .0 m and EUR139m , as compared to the corresponding quarter in 2006 .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[35] Diluted earnings per share ( EPS ) rose to EUR 3.68 from EUR 0.50 .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[36] Finnish cutlery and hand tools maker Fiskars Oyj Abp ( HEL : FISAS ) said today its net profit rose to EUR 24.1 million ( USD 33.6 m ) in the third quarter of 2010 from EUR 17.9 million a year earlier .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[37] HELSINKI ( AFX ) - Retail and wholesale group Kesko reported net sales of 659.4 mln eur for February , an increase of 10.8 pct year-on-year .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[38] In December alone , the members of the Lithuanian Brewers ' Association sold a total of 20.3 million liters of beer , an increase of 1.9 percent from the sales of 19.92 million liters in December 2004 .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[39] In Sweden , operating profit for the period under review totaled EUR 3.4 mn , up 30.8 % from the corresponding period in 2005 .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[40] In the second quarter of 2010 , the group 's net profit rose to EUR 3.1 million from EUR 2.5 million in April-June 2009 .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[41] Its market share widened to 48.51 percent from 48.31 percent a year earlier .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[42] ( ADP News ) - Feb 11 , 2009 - Finnish management software solutions provider Ixonos Oyj ( HEL : XNS1V ) said today its net profit rose to EUR 3.5 million ( USD 4.5 m ) for 2008 from EUR 3.1 million for 2007 .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[43] ( ADP News ) - Feb 6 , 2009 - Finnish fishing tackle company Rapala VMC Corp ( HEL : RAP1V ) said today its net profit rose to EUR 19.2 million ( USD 24.6 m ) for 2008 from EUR 17.5 million for 2007 .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[44] ( ADP News ) - Oct 29 , 2008 - Finnish lifting equipment maker Konecranes Oyj ( OMX : KCR1V ) said today that its net profit rose to EUR 116.6 million ( USD 149.1 m ) in the first nine months of 2008 from EUR 73.6 million for the s\n",
            "    真實標籤：Positive\n",
            "\n",
            "[45] Net interest income increased by 4.5 % to EUR 31.4 mn from EUR 30.0 mn in 2004 .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[46] Operating income rose to EUR 696.4 mn from EUR 600.3 mn in 2009 .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[47] Operating profit rose to EUR 13.5 mn from EUR 9.7 mn in the corresponding period in 2006 .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[48] Operating profit rose to EUR 4.7 mn from EUR 4.6 mn .\n",
            "    真實標籤：Positive\n",
            "\n",
            "[49] Operating profit totaled EUR 37,7 mn , up slightly from EUR 37.2 mn in the corresponding period in 2006 .\n",
            "    真實標籤：Positive\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 先展示測試集中的句子，讓學生了解有哪些資料\n",
        "print(\"\\n=== 測試集句子展示 ===\")\n",
        "print(\"以下是測試集中的前 50 個句子：\\n\")\n",
        "\n",
        "label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
        "\n",
        "for i in range(50):\n",
        "    sentence = test_dataset[i][\"sentence\"]\n",
        "    label = test_dataset[i][\"label\"]\n",
        "    print(f\"[{i}] {sentence}\")\n",
        "    print(f\"    真實標籤：{label_map[label]}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fR2jaBisoy33"
      },
      "source": [
        "因為目前找到的內容都沒有negative的，所以透過以下程式去尋找，因為我想測試三種情狀是否都能判斷成功。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJFg-h-J8V2v",
        "outputId": "8a2b2661-2b8c-4e5b-a404-2254f5fe903f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Negative 類別資料檢查結果 ===\n",
            "✅ 找到了！整個測試集共有 124 筆 'Negative' (標籤 0) 資料。\n",
            "----------------------------------------\n",
            "   [範例]：One of the challenges in the oil production in the North Sea is scale formation that can plug pipelines and halt production .\n"
          ]
        }
      ],
      "source": [
        "# 假設你的資料集物件是 test_dataset\n",
        "label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
        "negative_count = 0\n",
        "found_negative = False\n",
        "example_sentence = None\n",
        "\n",
        "# 遍歷整個測試集\n",
        "for i in range(len(test_dataset)):\n",
        "    label = test_dataset[i][\"label\"]\n",
        "\n",
        "    if label == 0:\n",
        "        negative_count += 1\n",
        "        found_negative = True\n",
        "\n",
        "        # 儲存第一個找到的負面句子的範例\n",
        "        if example_sentence is None:\n",
        "            example_sentence = test_dataset[i][\"sentence\"]\n",
        "\n",
        "# 輸出結果\n",
        "print(\"\\n=== Negative 類別資料檢查結果 ===\")\n",
        "if found_negative:\n",
        "    print(f\"✅ 找到了！整個測試集共有 {negative_count} 筆 'Negative' (標籤 0) 資料。\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"   [範例]：{example_sentence}\")\n",
        "else:\n",
        "    print(\"❌ 警告：在整個測試集中，沒有找到任何 'Negative' (標籤 0) 的資料。\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nn0Pfbga8V2v",
        "outputId": "2adba77b-2ee8-43fe-8ea0-b045d796b2cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== 尋找第一筆 Negative 資料 ===\n",
            "✅ 找到了！第一筆 'Negative' 資料的索引是：104\n",
            "   句子內容：One of the challenges in the oil production in the North Sea is scale formation that can plug pipelines and halt production .\n",
            "   真實標籤：Negative\n"
          ]
        }
      ],
      "source": [
        "# 假設你的資料集物件是 test_dataset\n",
        "label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
        "\n",
        "print(\"\\n=== 尋找第一筆 Negative 資料 ===\")\n",
        "\n",
        "# 遍歷整個測試集\n",
        "for i in range(len(test_dataset)):\n",
        "    label = test_dataset[i][\"label\"]\n",
        "\n",
        "    # 檢查標籤是否為 0 (Negative)\n",
        "    if label == 0:\n",
        "        sentence = test_dataset[i][\"sentence\"]\n",
        "\n",
        "        # 印出結果並跳出迴圈\n",
        "        print(f\"✅ 找到了！第一筆 'Negative' 資料的索引是：{i}\")\n",
        "        print(f\"   句子內容：{sentence}\")\n",
        "        print(f\"   真實標籤：{label_map[label]}\")\n",
        "\n",
        "        # 找到後立即停止迴圈\n",
        "        break\n",
        "else:\n",
        "    # 只有當迴圈正常結束（沒有執行到 break）時，才會執行 else 區塊\n",
        "    print(\"❌ 在整個測試集中，沒有找到任何 'Negative' (標籤 0) 的資料。\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkdpszUB8V2v",
        "outputId": "41495f93-0e7a-4e55-81b6-1889d71aab9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "你選擇測試的句子索引：[0, 9, 13, 26, 104]\n",
            "\n",
            "=== 開始預測 ===\n",
            "\n",
            "你選擇的 5 個句子的預測結果：\n",
            "\n",
            "1. 測試集索引 [0]\n",
            "   句子：TeliaSonera TLSN said the offer is in line with its strategy to increase its ownership in core business holdings and would strengthen Eesti Telekom 's offering to its customers .\n",
            "   真實標籤：Positive\n",
            "   預測標籤：Positive\n",
            "   結果：✓ 正確\n",
            "\n",
            "2. 測試集索引 [9]\n",
            "   句子：Nyrstar has also agreed to supply to Talvivaara up to 150,000 tonnes of sulphuric acid per annum for use in Talvivaara 's leaching process during the period of supply of the zinc in concentrate .\n",
            "   真實標籤：Positive\n",
            "   預測標籤：Neutral\n",
            "   結果：✗ 錯誤\n",
            "\n",
            "3. 測試集索引 [13]\n",
            "   句子：Tiimari operates 194 stores in six countries -- including its core Finnish market -- and generated a turnover of 76.5 mln eur in 2005 .\n",
            "   真實標籤：Neutral\n",
            "   預測標籤：Neutral\n",
            "   結果：✓ 正確\n",
            "\n",
            "4. 測試集索引 [26]\n",
            "   句子：A structures BIM ( building information modeling ) software from Tekla , a model-based software provider , has been adopted for the construction management and delivery of a university project in Abu Dhabi .\n",
            "   真實標籤：Neutral\n",
            "   預測標籤：Positive\n",
            "   結果：✗ 錯誤\n",
            "\n",
            "5. 測試集索引 [104]\n",
            "   句子：One of the challenges in the oil production in the North Sea is scale formation that can plug pipelines and halt production .\n",
            "   真實標籤：Negative\n",
            "   預測標籤：Negative\n",
            "   結果：✓ 正確\n",
            "\n",
            "準確率：3/5 = 60.00%\n"
          ]
        }
      ],
      "source": [
        "# 請從上面顯示的測試集句子中，選擇你想要測試的句子索引\n",
        "#\n",
        "# 選擇特定的句子索引\n",
        "# 例如：test_indices = [0, 5, 10, 15, 19]\n",
        "#\n",
        "# 提示：\n",
        "# - 可以選擇不同情緒的句子來測試\n",
        "# - 觀察模型在哪些句子上預測正確，哪些預測錯誤\n",
        "# - 思考為什麼模型會預測錯誤\n",
        "\n",
        "test_indices = [0, 9, 13, 26, 104]  # 請修改這裡！選擇你想測試的句子索引\n",
        "\n",
        "print(f\"\\n你選擇測試的句子索引：{test_indices}\")\n",
        "# ============================================================================\n",
        "\n",
        "# 根據選擇的索引，取得測試句子\n",
        "test_texts = [test_dataset[i][\"sentence\"] for i in test_indices]\n",
        "true_labels = [test_dataset[i][\"label\"] for i in test_indices]\n",
        "\n",
        "print(\"\\n=== 開始預測 ===\\n\")\n",
        "\n",
        "\"\"\"\n",
        "預測流程說明：\n",
        "\n",
        "1. 取得測試句子和真實標籤\n",
        "   - 根據選擇的索引提取句子\n",
        "\n",
        "2. Tokenization\n",
        "   - 將句子轉換成模型輸入格式\n",
        "   - return_tensors=\"pt\" 返回 PyTorch tensor\n",
        "\n",
        "3. 模型推理\n",
        "   - 將 tokenized 資料輸入模型\n",
        "   - 得到 logits（未標準化的分數）\n",
        "\n",
        "4. 預測類別\n",
        "   - 使用 argmax 取得分數最高的類別\n",
        "   - 轉換回 CPU 以便後續處理\n",
        "\n",
        "5. 結果展示\n",
        "   - 顯示原始句子、真實標籤、預測標籤\n",
        "   - 標記預測是否正確\n",
        "   - 計算準確率\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize 測試句子\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
        "outputs = model(**test_encodings)\n",
        "\n",
        "# 取得預測結果\n",
        "preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()  # 將結果轉回 CPU 以便處理\n",
        "\n",
        "# 顯示預測結果\n",
        "predicted_labels = [label_map[pred] for pred in preds]\n",
        "true_labels_text = [label_map[label] for label in true_labels]\n",
        "\n",
        "# 計算準確率\n",
        "correct = sum([1 for true, pred in zip(true_labels_text, predicted_labels) if true == pred])\n",
        "accuracy = correct / len(test_texts) * 100\n",
        "\n",
        "print(f\"你選擇的 {len(test_texts)} 個句子的預測結果：\\n\")\n",
        "\n",
        "for i, (idx, text, true_label, pred_label) in enumerate(zip(test_indices, test_texts, true_labels_text, predicted_labels)):\n",
        "    correct_mark = \"✓ 正確\" if true_label == pred_label else \"✗ 錯誤\"\n",
        "    print(f\"{i+1}. 測試集索引 [{idx}]\")\n",
        "    print(f\"   句子：{text}\")\n",
        "    print(f\"   真實標籤：{true_label}\")\n",
        "    print(f\"   預測標籤：{pred_label}\")\n",
        "    print(f\"   結果：{correct_mark}\\n\")\n",
        "\n",
        "print(f\"準確率：{correct}/{len(test_texts)} = {accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELE2YyJXtL3m"
      },
      "source": [
        "因為目前中立跟負面情緒的句子太少不夠訓練，所以用以下方式找出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhSv0ojA8V2v",
        "outputId": "ac1b78fc-1e29-487b-8167-91f1b05de6bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== 尋找 Negative (0) 與 Neutral (1) 資料各 5 筆 ===\n",
            "目標：各 5 筆\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Negative 範例 (5 筆) ---\n",
            "[索引 104 ] One of the challenges in the oil production in the North Sea is scale formation that can plug pipelines and halt production .\n",
            "[索引 137 ] Profit before taxes amounted to EUR 56.5 mn , down from EUR 232.9 mn a year ago .\n",
            "[索引 140 ] Also construction expenses have gone up in Russia .\n",
            "[索引 303 ] Finnish software and hardware developer Elektrobit Oyj HEL : EBG1V , or EB , said today it will temporarily lay off up to 200 people for a maximum of 90 day in Finland , aiming to achieve cost savings of EUR 1.7 million USD 2m in the second half of 2010 .\n",
            "[索引 331 ] JP Morgan expects that Scala will lower Nobel Biocare 's growth forecast for 2007 from the current guidance of 23-25 pct , as well as the operating margin target from the current 34-35 pct .\n",
            "\n",
            "--- Neutral 範例 (5 筆) ---\n",
            "[索引 13  ] Tiimari operates 194 stores in six countries -- including its core Finnish market -- and generated a turnover of 76.5 mln eur in 2005 .\n",
            "[索引 26  ] A structures BIM ( building information modeling ) software from Tekla , a model-based software provider , has been adopted for the construction management and delivery of a university project in Abu Dhabi .\n",
            "[索引 59  ] Vacon will supply drives to Ruselprom in the power range from 200 kW up to 2 MW .\n",
            "[索引 64  ] Finnish Componenta has published its new long-term strategy for the period 2011-2015 with the aim of growing together with its customers .\n",
            "[索引 79  ] As previously announced , GeoSentric Oyj entered into financing agreements with its lead investor on June 30 , 2010 enabling the Company to receive financing up to the aggregate amount of 6M .\n",
            "\n",
            "✅ 尋找完成。\n"
          ]
        }
      ],
      "source": [
        "# 假設你的資料集物件是 test_dataset\n",
        "label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
        "target_labels = [0, 1]  # 0: Negative, 1: Neutral\n",
        "target_count = 5       # 每種標籤需要的數量\n",
        "found_samples = {0: [], 1: []}\n",
        "total_needed = 2 * target_count\n",
        "\n",
        "print(\"\\n=== 尋找 Negative (0) 與 Neutral (1) 資料各 5 筆 ===\")\n",
        "print(f\"目標：各 {target_count} 筆\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 遍歷整個測試集\n",
        "for i in range(len(test_dataset)):\n",
        "    label = test_dataset[i][\"label\"]\n",
        "    sentence = test_dataset[i][\"sentence\"]\n",
        "\n",
        "    # 檢查是否為目標標籤且數量尚未滿 5 筆\n",
        "    if label in target_labels and len(found_samples[label]) < target_count:\n",
        "\n",
        "        # 儲存找到的句子和索引\n",
        "        found_samples[label].append({\n",
        "            \"index\": i,\n",
        "            \"sentence\": sentence\n",
        "        })\n",
        "\n",
        "        # 檢查是否已找到全部所需的樣本\n",
        "        current_found = len(found_samples[0]) + len(found_samples[1])\n",
        "        if current_found == total_needed:\n",
        "            break # 數量已滿，停止迴圈\n",
        "\n",
        "# 輸出結果\n",
        "for label, samples in found_samples.items():\n",
        "    label_name = label_map[label]\n",
        "    print(f\"\\n--- {label_name} 範例 ({len(samples)} 筆) ---\")\n",
        "\n",
        "    if not samples:\n",
        "        print(f\"❌ 警告：未找到任何 {label_name} 類別的資料。\")\n",
        "    else:\n",
        "        for sample in samples:\n",
        "            print(f\"[索引 {sample['index']:<4}] {sample['sentence']}\")\n",
        "\n",
        "print(\"\\n✅ 尋找完成。\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}